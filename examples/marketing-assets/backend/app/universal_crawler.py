"""
Universal Web Crawler - Crawl báº¥t ká»³ website nÃ o vá»›i chiáº¿n lÆ°á»£c tá»± Ä‘á»™ng
Há»— trá»£:
- Static sites (server-side rendering)
- React/Vue/Angular SPA (client-side rendering)
- Hybrid sites
- Auto-detect vÃ  fallback strategies
"""

import asyncio, os, re, hashlib, json
from urllib.parse import urlparse, urljoin
from bs4 import BeautifulSoup
from crawl4ai import (
    AsyncWebCrawler,
    BrowserConfig,
    CrawlerRunConfig,
    CacheMode,
)

class CrawlStrategy:
    """Enum cho cÃ¡c chiáº¿n lÆ°á»£c crawl"""
    DIRECT = "direct"  # Truy cáº­p trá»±c tiáº¿p URL
    SPA_WITH_DELAY = "spa_with_delay"  # SPA vá»›i delay Ä‘á»ƒ JS render
    NAVIGATE_FROM_HOME = "navigate_from_home"  # Navigate tá»« trang chá»§

class CrawlResult:
    """Káº¿t quáº£ crawl má»™t trang"""
    def __init__(self, url, success=False, strategy=None):
        self.url = url
        self.success = success
        self.strategy = strategy
        self.title = ""
        self.content = ""
        self.description = ""
        self.html_size = 0
        self.cleaned_text_size = 0
        self.is_404 = False
        
class UniversalCrawler:
    """Crawler tá»•ng quÃ¡t cho má»i loáº¡i website"""
    
    def __init__(self, base_url, output_dir="crawled_data", config=None):
        self.base_url = base_url.rstrip('/')
        self.domain = urlparse(base_url).netloc
        self.output_dir = output_dir
        self.config = config or {}
        
        # Cáº¥u hÃ¬nh ngÆ°á»¡ng
        self.min_content_length = self.config.get('min_content_length', 200)
        self.min_cleaned_ratio = self.config.get('min_cleaned_ratio', 0.1)  # 10% cá»§a HTML
        
        os.makedirs(output_dir, exist_ok=True)
        
    def slugify(self, text: str) -> str:
        """Chuyá»ƒn text thÃ nh slug an toÃ n cho tÃªn file (giá»¯ nguyÃªn nguyÃªn Ã¢m tiáº¿ng Viá»‡t thÃ nh ASCII).

        Cáº£i tiáº¿n: dÃ¹ng Unicode decomposition Ä‘á»ƒ loáº¡i bá» dáº¥u mÃ  KHÃ”NG máº¥t nguyÃªn Ã¢m.
        Kháº¯c phá»¥c lá»—i trÆ°á»›c Ä‘Ã¢y táº¡o slug nhÆ° "lin-h-vi-chng-ti" thay vÃ¬ "lien-he-voi-chung-toi".
        """
        import unicodedata
        if not text:
            return "page"
        # Chuáº©n hoÃ¡ & tÃ¡ch tá»• há»£p
        text_norm = unicodedata.normalize("NFD", text)
        out_chars = []
        for ch in text_norm:
            # Bá» cÃ¡c dáº¥u (combining marks)
            if unicodedata.category(ch) == "Mn":
                continue
            # Chuyá»ƒn Ä‘/Ä
            if ch == "Ä‘":
                ch = "d"
            elif ch == "Ä":
                ch = "D"
            out_chars.append(ch)
        ascii_text = "".join(out_chars)
        ascii_text = unicodedata.normalize("NFC", ascii_text).lower()
        # Giá»¯ láº¡i chá»¯ cÃ¡i, sá»‘, khoáº£ng tráº¯ng vÃ  dáº¥u '-'
        ascii_text = re.sub(r"[^a-z0-9\s-]", "", ascii_text)
        # Thu gá»n khoáº£ng tráº¯ng -> '-'
        ascii_text = re.sub(r"\s+", "-", ascii_text.strip())
        # Thu gá»n nhiá»u dáº¥u '-' liÃªn tiáº¿p
        ascii_text = re.sub(r"-+", "-", ascii_text)
        return ascii_text[:80] or "page"
    
    def url_hash(self, url: str) -> str:
        """Táº¡o hash 8 kÃ½ tá»± tá»« URL"""
        return hashlib.md5(url.encode("utf-8")).hexdigest()[:8]
    
    def extract_content(self, result, soup=None) -> dict:
        """TrÃ­ch xuáº¥t ná»™i dung tá»« káº¿t quáº£ crawl"""
        if soup is None:
            html = result.html or ""
            soup = BeautifulSoup(html, "html.parser")
        
        # Láº¥y tiÃªu Ä‘á»
        title = ""
        h1_tags = soup.find_all("h1")
        if h1_tags:
            for h1 in h1_tags:
                text = h1.get_text(strip=True)
                # Bá» qua h1 lÃ  logo/site name
                if text and len(text) > 3 and text.lower() not in [self.domain.lower()]:
                    title = text
                    break
            if not title and h1_tags:
                title = h1_tags[0].get_text(strip=True)
        
        # Fallback tá»« metadata
        if not title and result.metadata:
            meta = result.metadata
            title = meta.get("title") or meta.get("og:title") or ""
            
        if not title:
            title_tag = soup.find("title")
            if title_tag:
                title = title_tag.get_text(strip=True)
                
        # Láº¥y ná»™i dung chÃ­nh
        content = ""
        
        # Thá»­ cÃ¡c selector phá»• biáº¿n
        main_selectors = [
            "main", "article", "[role='main']",
            ".main-content", ".content", "#content",
            ".post-content", ".entry-content",
            ".article-content", ".page-content"
        ]
        
        for selector in main_selectors:
            main_tag = soup.select_one(selector)
            if main_tag:
                content = main_tag.get_text("\n", strip=True)
                if len(content) > 100:
                    break
        
        # Fallback: tá»« body (loáº¡i bá» header/footer/nav)
        if not content or len(content) < 100:
            body = soup.find("body")
            if body:
                body_copy = BeautifulSoup(str(body), "html.parser")
                for tag in body_copy.find_all(["header", "footer", "nav", "aside"]):
                    tag.decompose()
                content = body_copy.get_text("\n", strip=True)
        
        # Description
        description = ""
        if result.metadata:
            description = result.metadata.get("description") or result.metadata.get("og:description") or ""
        
        # Kiá»ƒm tra 404
        page_text = soup.get_text().lower()
        is_404 = any([
            "page not found" in page_text,
            "404" in title.lower(),
            "not found" in title.lower() and len(content) < 500
        ])
        
        return {
            "title": title or "KhÃ´ng cÃ³ tiÃªu Ä‘á»",
            "content": content,
            "description": description,
            "is_404": is_404,
            "html_size": len(result.html or ""),
            "cleaned_text_size": len(content)
        }
    
    def evaluate_content_quality(self, extracted_data) -> bool:
        """ÄÃ¡nh giÃ¡ cháº¥t lÆ°á»£ng ná»™i dung Ä‘Ã£ crawl"""
        # 1) Loáº¡i trá»« trang 404
        if extracted_data["is_404"]:
            return False

        text_len = extracted_data["cleaned_text_size"]
        html_size = extracted_data["html_size"] or 0
        ratio = (text_len / html_size) if html_size else 1.0

        # 2) Äá»™ dÃ i tá»‘i thiá»ƒu tuyá»‡t Ä‘á»‘i
        if text_len < self.min_content_length:
            return False

        # 3) Chiáº¿n lÆ°á»£c Ä‘Ã¡nh giÃ¡ linh hoáº¡t theo ratio:
        #    - Náº¿u ná»™i dung Ä‘á»§ dÃ i (> 2 * min_content_length) thÃ¬ bá» qua ratio (trang nhiá»u markup nhÆ° SPA)
        #    - Náº¿u ratio dÆ°á»›i ngÆ°á»¡ng nhÆ°ng váº«n > ngÆ°á»¡ng ná»›i lá»ng (half) vÃ  text Ä‘á»§ dÃ i thÃ¬ cháº¥p nháº­n.
        #    - NgÆ°á»¡ng tá»‘i thiá»ƒu tuyá»‡t Ä‘á»‘i cho ratio lÃ  0.02 (trÃ¡nh nháº­n trang háº§u nhÆ° toÃ n script).
        if ratio < self.min_cleaned_ratio:
            relaxed_threshold = max(0.02, self.min_cleaned_ratio / 2)
            if text_len >= self.min_content_length * 2:
                return True  # Ä‘á»§ dÃ i, cháº¥p nháº­n
            if ratio >= relaxed_threshold and text_len >= int(self.min_content_length * 1.2):
                return True  # ná»›i lá»ng
            return False

        return True
    
    async def crawl_with_strategy(self, crawler, url: str, strategy: str, base_config: CrawlerRunConfig) -> CrawlResult:
        """Crawl vá»›i má»™t chiáº¿n lÆ°á»£c cá»¥ thá»ƒ"""
        result = CrawlResult(url, strategy=strategy)
        
        try:
            if strategy == CrawlStrategy.DIRECT:
                # Crawl trá»±c tiáº¿p, khÃ´ng delay
                config = CrawlerRunConfig(
                    cache_mode=CacheMode.BYPASS,
                    word_count_threshold=0,
                    verbose=False,
                    page_timeout=30000,
                    delay_before_return_html=1.0,
                )
                crawl_result = await crawler.arun(url=url, config=config)
                
            elif strategy == CrawlStrategy.SPA_WITH_DELAY:
                # SPA vá»›i delay dÃ i hÆ¡n
                config = CrawlerRunConfig(
                    cache_mode=CacheMode.BYPASS,
                    word_count_threshold=0,
                    verbose=False,
                    page_timeout=60000,
                    delay_before_return_html=5.0,
                )
                crawl_result = await crawler.arun(url=url, config=config)
                
            elif strategy == CrawlStrategy.NAVIGATE_FROM_HOME:
                # Navigate tá»« trang chá»§
                if url == self.base_url:
                    # Náº¿u Ä‘ang á»Ÿ trang chá»§, dÃ¹ng SPA_WITH_DELAY
                    return await self.crawl_with_strategy(crawler, url, CrawlStrategy.SPA_WITH_DELAY, base_config)
                
                # Táº¡o JS code Ä‘á»ƒ navigate
                path = url.replace(self.base_url, '').lstrip('/')
                js_click_code = f"""
                // Äá»£i trang load
                await new Promise(resolve => setTimeout(resolve, 2000));
                
                // TÃ¬m link vÃ  click
                const links = Array.from(document.querySelectorAll('a[href*="/{path}"]'));
                if (links.length > 0) {{
                    links[0].click();
                    await new Promise(resolve => setTimeout(resolve, 3000));
                }} else {{
                    // Thá»­ tÃ¬m link vá»›i path Ä‘áº§y Ä‘á»§
                    const fullLinks = Array.from(document.querySelectorAll('a[href="{url}"]'));
                    if (fullLinks.length > 0) {{
                        fullLinks[0].click();
                        await new Promise(resolve => setTimeout(resolve, 3000));
                    }}
                }}
                """
                
                config = CrawlerRunConfig(
                    cache_mode=CacheMode.BYPASS,
                    word_count_threshold=0,
                    verbose=False,
                    page_timeout=60000,
                    delay_before_return_html=5.0,
                    js_code=js_click_code,
                )
                
                # Load trang chá»§ vá»›i JS click
                crawl_result = await crawler.arun(url=self.base_url, config=config)
            
            else:
                raise ValueError(f"Unknown strategy: {strategy}")
            
            if not crawl_result.success:
                return result
            
            # TrÃ­ch xuáº¥t vÃ  Ä‘Ã¡nh giÃ¡ ná»™i dung
            extracted = self.extract_content(crawl_result)
            
            result.success = self.evaluate_content_quality(extracted)
            result.title = extracted["title"]
            result.content = extracted["content"]
            result.description = extracted["description"]
            result.html_size = extracted["html_size"]
            result.cleaned_text_size = extracted["cleaned_text_size"]
            result.is_404 = extracted["is_404"]
            
            return result
            
        except Exception as e:
            print(f"   âŒ Error with {strategy}: {str(e)}")
            return result
    
    async def crawl_url_with_fallback(self, crawler, url: str, base_config: CrawlerRunConfig) -> CrawlResult:
        """Crawl má»™t URL vá»›i cÃ¡c chiáº¿n lÆ°á»£c fallback tá»± Ä‘á»™ng"""
        
        print(f"\n{'='*70}")
        print(f"ğŸ” Crawling: {url}")
        print('='*70)
        
        # Thá»­ cÃ¡c chiáº¿n lÆ°á»£c theo thá»© tá»±
        strategies = [
            CrawlStrategy.DIRECT,
            CrawlStrategy.SPA_WITH_DELAY,
            CrawlStrategy.NAVIGATE_FROM_HOME
        ]
        
        for i, strategy in enumerate(strategies):
            print(f"   Trying strategy {i+1}/{len(strategies)}: {strategy}...", end=" ")
            
            result = await self.crawl_with_strategy(crawler, url, strategy, base_config)
            
            if result.success:
                print(f"âœ… Success!")
                print(f"   ğŸ“„ Title: {result.title}")
                print(f"   ğŸ“Š Content: {result.cleaned_text_size:,} chars")
                return result
            else:
                print(f"âŒ Failed")
                if result.is_404:
                    print(f"      (Page not found)")
                elif result.cleaned_text_size < self.min_content_length:
                    print(f"      (Content too short: {result.cleaned_text_size} chars)")
                else:
                    if result.html_size:
                        ratio = result.cleaned_text_size / result.html_size
                        print(f"      (Low text/html ratio: {ratio:.3f} < {self.min_cleaned_ratio})")
                    else:
                        print("      (Empty HTML)")
        
        # Táº¥t cáº£ chiáº¿n lÆ°á»£c Ä‘á»u tháº¥t báº¡i
        print(f"   âš ï¸  All strategies failed for {url}")
        return result
    
    def save_result(self, result: CrawlResult):
        """LÆ°u káº¿t quáº£ vÃ o file"""
        slug = self.slugify(result.title)
        filename = f"{slug}-{self.url_hash(result.url)}.md"
        filepath = os.path.join(self.output_dir, filename)
        
        with open(filepath, "w", encoding="utf-8") as f:
            f.write("---\n")
            f.write(f'title: "{result.title.replace(chr(34), chr(39))}"\n')
            f.write(f"url: {result.url}\n")
            f.write(f"strategy: {result.strategy}\n")
            if result.description:
                f.write(f'description: "{result.description.replace(chr(34), chr(39))}"\n')
            if result.is_404:
                f.write("status: 404\n")
            f.write("---\n\n")
            f.write(f"# {result.title}\n\n")
            if result.description:
                f.write(f"_{result.description}_\n\n")
            f.write(result.content)
        
        return filename
    
    async def crawl_urls(self, urls: list):
        """Crawl danh sÃ¡ch URLs"""
        
        print(f"ğŸš€ Universal Crawler")
        print(f"ğŸ“ Base URL: {self.base_url}")
        print(f"ğŸ¯ Total URLs: {len(urls)}")
        print(f"ğŸ“ Output: {self.output_dir}")
        
        browser_cfg = BrowserConfig(
            browser_type="chromium",
            headless=True,
            verbose=False
        )
        
        base_config = CrawlerRunConfig(
            cache_mode=CacheMode.BYPASS,
            word_count_threshold=0,
            verbose=False,
        )
        
        results = []
        
        async with AsyncWebCrawler(config=browser_cfg) as crawler:
            for url in urls:
                result = await self.crawl_url_with_fallback(crawler, url, base_config)
                
                if result.success:
                    filename = self.save_result(result)
                    print(f"   ğŸ’¾ Saved: {filename}")
                    results.append(result)
                else:
                    print(f"   âš ï¸  Skipped (no valid content)")
                
                # Delay giá»¯a cÃ¡c requests
                await asyncio.sleep(1)
        
        # Thá»‘ng kÃª
        print(f"\n{'='*70}")
        print("ğŸ“Š SUMMARY")
        print('='*70)
        print(f"Total URLs: {len(urls)}")
        print(f"Successfully crawled: {len(results)}")
        print(f"Failed: {len(urls) - len(results)}")
        
        if results:
            print(f"\nâœ… Crawled pages:")
            strategy_counts = {}
            for r in results:
                strategy_counts[r.strategy] = strategy_counts.get(r.strategy, 0) + 1
                print(f"   - {r.title[:60]}: {r.cleaned_text_size:,} chars [{r.strategy}]")
            
            print(f"\nğŸ“ˆ Strategy usage:")
            for strategy, count in strategy_counts.items():
                print(f"   - {strategy}: {count} pages")
        
        print(f"\nğŸ“ Output directory: {self.output_dir}")
        
        return results


# VÃ­ dá»¥ sá»­ dá»¥ng
async def main():
    """Demo sá»­ dá»¥ng Universal Crawler"""
    
    import sys
    
    # Nháº­n tham sá»‘ Ä‘áº§u vÃ o: cÃ³ thá»ƒ lÃ  domain root hoáº·c má»™t URL cá»¥ thá»ƒ.
    if len(sys.argv) > 1:
        input_url = sys.argv[1].rstrip('/')
    else:
        input_url = "https://trieuvu.netlify.app"

    parsed = urlparse(input_url)
    root_base = f"{parsed.scheme}://{parsed.netloc}".rstrip('/')

    # Náº¿u ngÆ°á»i dÃ¹ng truyá»n má»™t URL cÃ³ path (vd: /lien-he) thÃ¬ váº«n dÃ¹ng root domain lÃ m base.
    # Khi Ä‘Ã³ ta crawl cáº£ trang chá»§ rá»“i má»›i thá»­ URL Ä‘Ã­ch Ä‘á»ƒ há»— trá»£ SPA navigation.
    if parsed.path and parsed.path not in ('', '/'):
        base_url = root_base
        urls = [root_base + '/', input_url]
    else:
        base_url = root_base
        # CÃ¡c bundle preset cho má»™t sá»‘ site demo
        if base_url == "https://trieuvu.netlify.app":
            urls = [
                base_url + '/',
                base_url + '/gioi-thieu',
                base_url + '/dich-vu',
                base_url + '/bang-gia',
                base_url + '/tin-tuc',
                base_url + '/lien-he',
            ]
        elif base_url == "https://tuesy.net":
            urls = [
                "https://tuesy.net/le-thang-bay-cho-nhung-oan-hon-phieu-bat/",
                "https://tuesy.net/phuong-tien-thien-xao/",
                "https://tuesy.net/du-gia-bo-tat-gioi/",
            ]
        else:
            urls = [base_url + '/']
    
    # Cáº¥u hÃ¬nh
    config = {
        'min_content_length': 200,
        # Giáº£m ratio máº·c Ä‘á»‹nh Ä‘á»ƒ phÃ¹ há»£p cÃ¡c SPA nhiá»u markup
        'min_cleaned_ratio': 0.05,
    }
    
    # Táº¡o output directory dá»±a trÃªn domain
    domain = urlparse(base_url).netloc.replace('.', '_')
    output_dir = f"crawled_{domain}"
    
    # Khá»Ÿi táº¡o vÃ  cháº¡y crawler
    crawler = UniversalCrawler(base_url, output_dir, config)
    results = await crawler.crawl_urls(urls)
    
    return results


if __name__ == "__main__":
    asyncio.run(main())
